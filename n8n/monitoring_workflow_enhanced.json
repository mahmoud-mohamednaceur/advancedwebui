{
    "nodes": [
        {
            "parameters": {
                "content": "## Knowledge Base Monitoring V2\n\n**Enhanced Queries Include:**\n- ðŸ“Š Health Status Calculation (HEALTHY/WARNING/CRITICAL/SYNCING)\n- ðŸ”„ Pipeline Success Rates\n- âœ¨ Enhancement Coverage %\n- ðŸ” Smart Duplicate Detection (content-hash based)\n- ðŸ“ Cross-File Duplicate Detection\n- âš ï¸ Expanded Quality Issue Types\n- ðŸ—„ï¸ Vector Store Integrity Checks\n- ðŸ“ˆ Detailed Size Distribution with Quartiles",
                "height": 1500,
                "width": 2100
            },
            "type": "n8n-nodes-base.stickyNote",
            "typeVersion": 1,
            "position": [
                -1504,
                1676
            ],
            "id": "66607fd5-81ee-47b4-a23e-a28a1b53da95",
            "name": "Sticky Note17"
        },
        {
            "parameters": {
                "options": {}
            },
            "type": "n8n-nodes-base.respondToWebhook",
            "typeVersion": 1.4,
            "position": [
                48,
                1920
            ],
            "id": "f31ad120-9ff5-4258-8205-5c9e48baf744",
            "name": "Respond to Webhook13"
        },
        {
            "parameters": {
                "options": {}
            },
            "type": "n8n-nodes-base.respondToWebhook",
            "typeVersion": 1.4,
            "position": [
                -80,
                2096
            ],
            "id": "6622781e-4365-4fcd-a83c-2cec206725b9",
            "name": "Respond to Webhook14"
        },
        {
            "parameters": {
                "options": {}
            },
            "type": "n8n-nodes-base.respondToWebhook",
            "typeVersion": 1.4,
            "position": [
                -80,
                2256
            ],
            "id": "88ad3d21-c098-40da-a843-d6edac72037d",
            "name": "Respond to Webhook15"
        },
        {
            "parameters": {
                "options": {}
            },
            "type": "n8n-nodes-base.respondToWebhook",
            "typeVersion": 1.4,
            "position": [
                -80,
                2448
            ],
            "id": "d579a744-af48-4b48-843a-a3617beb8684",
            "name": "Respond to Webhook16"
        },
        {
            "parameters": {
                "options": {}
            },
            "type": "n8n-nodes-base.respondToWebhook",
            "typeVersion": 1.4,
            "position": [
                -48,
                2704
            ],
            "id": "6b89ce6d-c94f-439f-8ff5-047fa30eda43",
            "name": "Respond to Webhook17"
        },
        {
            "parameters": {
                "options": {}
            },
            "type": "n8n-nodes-base.respondToWebhook",
            "typeVersion": 1.4,
            "position": [
                -48,
                2900
            ],
            "id": "respond-pipeline-status",
            "name": "Respond to Webhook - Pipeline"
        },
        {
            "parameters": {
                "options": {}
            },
            "type": "n8n-nodes-base.respondToWebhook",
            "typeVersion": 1.4,
            "position": [
                -48,
                3100
            ],
            "id": "respond-file-status",
            "name": "Respond to Webhook - Files"
        },
        {
            "parameters": {
                "operation": "executeQuery",
                "query": "-- Enhanced Duplicate Detection with Content Hash & Impact Assessment\nSELECT \n    crt.notebook_id,\n    n.notebook_title,\n    MD5(LOWER(TRIM(crt.original_chunk))) AS content_hash,\n    LEFT(crt.original_chunk, 200) AS chunk_preview,\n    COUNT(*) AS duplicate_count,\n    COUNT(DISTINCT crt.file_id) AS files_affected,\n    ARRAY_AGG(DISTINCT crt.file_name) AS file_names,\n    ARRAY_AGG(crt.chunk_id) AS chunk_ids,\n    ARRAY_AGG(crt.file_id) AS file_ids,\n    MIN(crt.created_at) AS first_created,\n    MAX(crt.created_at) AS last_created,\n    CASE \n        WHEN COUNT(*) > 5 THEN 'HIGH'\n        WHEN COUNT(*) > 2 THEN 'MEDIUM'\n        ELSE 'LOW'\n    END AS impact_level,\n    CASE \n        WHEN COUNT(DISTINCT crt.file_id) > 1 THEN true\n        ELSE false\n    END AS is_cross_file_duplicate\nFROM contextual_retrieval_table crt\nJOIN notebook n ON crt.notebook_id = n.notebook_id\nWHERE crt.original_chunk IS NOT NULL AND crt.original_chunk != ''\nGROUP BY crt.notebook_id, n.notebook_title, MD5(LOWER(TRIM(crt.original_chunk))), LEFT(crt.original_chunk, 200)\nHAVING COUNT(*) > 1\nORDER BY duplicate_count DESC, n.notebook_title\nLIMIT 100;",
                "options": {}
            },
            "type": "n8n-nodes-base.postgres",
            "typeVersion": 2.6,
            "position": [
                -688,
                1920
            ],
            "id": "aa939409-6bc0-494f-89b3-9401ad38c91d",
            "name": "Duplicate Chunks Query",
            "alwaysOutputData": true,
            "credentials": {
                "postgres": {
                    "id": "BkpXZhgdJu6BdpjK",
                    "name": "Local Postgres"
                }
            }
        },
        {
            "parameters": {
                "operation": "executeQuery",
                "query": "-- Enhanced Health Dashboard with Computed Status & Rates\nSELECT \n    n.notebook_id,\n    n.notebook_title,\n    n.number_of_documents,\n    n.created_at,\n    n.updated_at,\n    \n    -- Document Records Stats\n    COALESCE(dr.doc_count, 0) AS actual_document_records,\n    COALESCE(dr.tabular_count, 0) AS tabular_documents,\n    COALESCE(dr.non_tabular_count, 0) AS non_tabular_documents,\n    \n    -- Chunk Stats\n    COALESCE(crt.total_chunks, 0) AS total_chunks,\n    COALESCE(crt.pending_chunks, 0) AS pending_chunks,\n    COALESCE(crt.completed_chunks, 0) AS completed_chunks,\n    COALESCE(crt.failed_chunks, 0) AS failed_chunks,\n    COALESCE(crt.enhanced_chunks, 0) AS enhanced_chunks,\n    \n    -- Enhancement Coverage\n    CASE \n        WHEN COALESCE(crt.total_chunks, 0) = 0 THEN 0\n        ELSE ROUND((COALESCE(crt.enhanced_chunks, 0)::DECIMAL / crt.total_chunks) * 100, 1)\n    END AS enhancement_coverage_pct,\n    \n    -- Job Stats\n    COALESCE(jobs.total_jobs, 0) AS total_jobs,\n    COALESCE(jobs.pending_jobs, 0) AS pending_jobs,\n    COALESCE(jobs.processing_jobs, 0) AS processing_jobs,\n    COALESCE(jobs.completed_jobs, 0) AS completed_jobs,\n    COALESCE(jobs.failed_jobs, 0) AS failed_jobs,\n    \n    -- Job Success Rate\n    CASE \n        WHEN COALESCE(jobs.total_jobs, 0) = 0 THEN 100\n        ELSE ROUND((COALESCE(jobs.completed_jobs, 0)::DECIMAL / jobs.total_jobs) * 100, 1)\n    END AS job_success_rate_pct,\n    \n    -- Vector Store Stats\n    COALESCE(docs.vector_count, 0) AS vector_documents,\n    COALESCE(docs.missing_embeddings, 0) AS vectors_missing_embedding,\n    \n    -- Duplicate Detection\n    COALESCE(dups.duplicate_chunk_groups, 0) AS duplicate_chunk_groups,\n    COALESCE(dups.total_duplicate_chunks, 0) AS total_duplicate_chunks,\n    \n    -- Chat Stats\n    COALESCE(chat.message_count, 0) AS chat_messages,\n    COALESCE(chat.unique_users, 0) AS unique_chat_users,\n    \n    -- Orphan Detection\n    COALESCE(orphans.orphan_vector_count, 0) AS orphan_vectors,\n    \n    -- Problematic Chunks Count\n    COALESCE(quality.problematic_count, 0) AS problematic_chunks,\n    \n    -- COMPUTED HEALTH STATUS\n    CASE\n        WHEN COALESCE(jobs.failed_jobs, 0) > COALESCE(jobs.total_jobs, 0) * 0.2 THEN 'CRITICAL'\n        WHEN COALESCE(crt.failed_chunks, 0) > COALESCE(crt.total_chunks, 0) * 0.1 THEN 'CRITICAL'\n        WHEN COALESCE(dups.duplicate_chunk_groups, 0) > COALESCE(crt.total_chunks, 0) * 0.15 THEN 'WARNING'\n        WHEN COALESCE(quality.problematic_count, 0) > 10 THEN 'WARNING'\n        WHEN COALESCE(orphans.orphan_vector_count, 0) > 0 THEN 'WARNING'\n        WHEN COALESCE(docs.missing_embeddings, 0) > 0 THEN 'WARNING'\n        WHEN COALESCE(jobs.pending_jobs, 0) + COALESCE(jobs.processing_jobs, 0) > 0 THEN 'SYNCING'\n        WHEN COALESCE(crt.pending_chunks, 0) > 0 THEN 'SYNCING'\n        ELSE 'HEALTHY'\n    END AS health_status\n    \nFROM notebook n\n\n-- Document Records\nLEFT JOIN (\n    SELECT \n        notebook_id,\n        COUNT(*) AS doc_count,\n        COUNT(*) FILTER (WHERE document_type = 'tabular') AS tabular_count,\n        COUNT(*) FILTER (WHERE document_type = 'non_tabular') AS non_tabular_count\n    FROM document_records\n    GROUP BY notebook_id\n) dr ON n.notebook_id = dr.notebook_id\n\n-- Chunks\nLEFT JOIN (\n    SELECT \n        notebook_id,\n        COUNT(*) AS total_chunks,\n        COUNT(*) FILTER (WHERE status = 'pending') AS pending_chunks,\n        COUNT(*) FILTER (WHERE status = 'completed') AS completed_chunks,\n        COUNT(*) FILTER (WHERE status = 'failed') AS failed_chunks,\n        COUNT(*) FILTER (WHERE enhanced_chunk IS NOT NULL AND enhanced_chunk != '') AS enhanced_chunks\n    FROM contextual_retrieval_table\n    GROUP BY notebook_id\n) crt ON n.notebook_id = crt.notebook_id\n\n-- Jobs\nLEFT JOIN (\n    SELECT \n        notebook_id,\n        COUNT(*) AS total_jobs,\n        COUNT(*) FILTER (WHERE status = 'pending') AS pending_jobs,\n        COUNT(*) FILTER (WHERE status = 'processing') AS processing_jobs,\n        COUNT(*) FILTER (WHERE status = 'completed') AS completed_jobs,\n        COUNT(*) FILTER (WHERE status = 'failed') AS failed_jobs\n    FROM notebook_file_jobs\n    GROUP BY notebook_id\n) jobs ON n.notebook_id = jobs.notebook_id\n\n-- Vector Documents with Missing Embedding Check\nLEFT JOIN (\n    SELECT \n        metadata->>'notebook_id' AS notebook_id,\n        COUNT(*) AS vector_count,\n        COUNT(*) FILTER (WHERE embedding IS NULL) AS missing_embeddings\n    FROM documents\n    WHERE metadata->>'notebook_id' IS NOT NULL\n    GROUP BY metadata->>'notebook_id'\n) docs ON n.notebook_id::TEXT = docs.notebook_id\n\n-- Duplicate Chunks\nLEFT JOIN (\n    SELECT \n        notebook_id,\n        COUNT(*) AS duplicate_chunk_groups,\n        SUM(cnt - 1) AS total_duplicate_chunks\n    FROM (\n        SELECT notebook_id, MD5(LOWER(TRIM(original_chunk))) AS hash, COUNT(*) AS cnt\n        FROM contextual_retrieval_table\n        WHERE original_chunk IS NOT NULL AND original_chunk != ''\n        GROUP BY notebook_id, MD5(LOWER(TRIM(original_chunk)))\n        HAVING COUNT(*) > 1\n    ) dup_groups\n    GROUP BY notebook_id\n) dups ON n.notebook_id = dups.notebook_id\n\n-- Chat Messages with Unique Users\nLEFT JOIN (\n    SELECT \n        notebook_id, \n        COUNT(*) AS message_count,\n        COUNT(DISTINCT user_id) AS unique_users\n    FROM chat_history\n    GROUP BY notebook_id\n) chat ON n.notebook_id = chat.notebook_id\n\n-- Orphan Vectors (vectors without chunks)\nLEFT JOIN (\n    SELECT \n        d.metadata->>'notebook_id' AS notebook_id,\n        COUNT(*) AS orphan_vector_count\n    FROM documents d\n    WHERE d.metadata->>'notebook_id' IS NOT NULL\n    AND NOT EXISTS (\n        SELECT 1 FROM contextual_retrieval_table crt \n        WHERE crt.chunk_id = d.metadata->>'chunk_id'\n    )\n    GROUP BY d.metadata->>'notebook_id'\n) orphans ON n.notebook_id::TEXT = orphans.notebook_id\n\n-- Problematic Chunks Count\nLEFT JOIN (\n    SELECT \n        notebook_id,\n        COUNT(*) AS problematic_count\n    FROM contextual_retrieval_table\n    WHERE original_chunk IS NULL \n       OR original_chunk = '' \n       OR LENGTH(original_chunk) < 50\n       OR LENGTH(TRIM(original_chunk)) = 0\n    GROUP BY notebook_id\n) quality ON n.notebook_id = quality.notebook_id\n\nORDER BY \n    CASE \n        WHEN COALESCE(jobs.failed_jobs, 0) > 0 THEN 1\n        WHEN COALESCE(jobs.pending_jobs, 0) + COALESCE(jobs.processing_jobs, 0) > 0 THEN 2\n        ELSE 3\n    END,\n    n.updated_at DESC;",
                "options": {}
            },
            "type": "n8n-nodes-base.postgres",
            "typeVersion": 2.6,
            "position": [
                -688,
                2256
            ],
            "id": "600822d2-19e5-4ce6-a52b-56438ba3fda6",
            "name": "Health Overview Query",
            "alwaysOutputData": true,
            "credentials": {
                "postgres": {
                    "id": "BkpXZhgdJu6BdpjK",
                    "name": "Local Postgres"
                }
            }
        },
        {
            "parameters": {
                "httpMethod": "POST",
                "path": "36907c26-cd49-4578-abe2-2e5d5933a687-Chunk-size-distribution-per-notebook",
                "responseMode": "responseNode",
                "options": {}
            },
            "type": "n8n-nodes-base.webhook",
            "typeVersion": 2.1,
            "position": [
                -1312,
                2448
            ],
            "id": "62a07c14-88b7-4c50-89b3-215860e69181",
            "name": "Chunk size distribution per notebook",
            "webhookId": "36907c26-cd49-4578-abe2-2e5d5933a687"
        },
        {
            "parameters": {
                "httpMethod": "POST",
                "path": "36907c26-cd49-4578-abe2-2e5d5933a687-Comprehensive-notebook-health-overview",
                "responseMode": "responseNode",
                "options": {}
            },
            "type": "n8n-nodes-base.webhook",
            "typeVersion": 2.1,
            "position": [
                -1296,
                2256
            ],
            "id": "1b19f131-8e04-4296-8027-2b8f8551c752",
            "name": "Comprehensive notebook health overview",
            "webhookId": "36907c26-cd49-4578-abe2-2e5d5933a687"
        },
        {
            "parameters": {
                "httpMethod": "POST",
                "path": "36907c26-cd49-4578-abe2-2e5d5933a687- duplicate-chunks-within-the-same-notebook",
                "responseMode": "responseNode",
                "options": {}
            },
            "type": "n8n-nodes-base.webhook",
            "typeVersion": 2.1,
            "position": [
                -1280,
                1920
            ],
            "id": "6ff7e2e1-dcd6-4d9a-b4bf-402f4e18af91",
            "name": "Find duplicate chunks",
            "webhookId": "36907c26-cd49-4578-abe2-2e5d5933a687"
        },
        {
            "parameters": {
                "httpMethod": "POST",
                "path": "36907c26-cd49-4578-abe2-2e5d5933a687-Find-duplicate-enhanced-chunks-within-the-same-notebook",
                "responseMode": "responseNode",
                "options": {}
            },
            "type": "n8n-nodes-base.webhook",
            "typeVersion": 2.1,
            "position": [
                -1296,
                2096
            ],
            "id": "9ed69a5a-b1ed-46fb-9686-eca6d72ddda6",
            "name": "Find duplicate enhanced chunks",
            "webhookId": "36907c26-cd49-4578-abe2-2e5d5933a687"
        },
        {
            "parameters": {
                "httpMethod": "POST",
                "path": "36907c26-cd49-4578-abe2-2e5d5933a687-Empty-or-problematic-chunks-detection",
                "responseMode": "responseNode",
                "options": {}
            },
            "type": "n8n-nodes-base.webhook",
            "typeVersion": 2.1,
            "position": [
                -1328,
                2704
            ],
            "id": "7c487b09-522e-4fe7-bc97-20e19820ad13",
            "name": "Problematic chunks detection",
            "webhookId": "36907c26-cd49-4578-abe2-2e5d5933a687"
        },
        {
            "parameters": {
                "httpMethod": "POST",
                "path": "36907c26-cd49-4578-abe2-2e5d5933a687-pipeline-status",
                "responseMode": "responseNode",
                "options": {}
            },
            "type": "n8n-nodes-base.webhook",
            "typeVersion": 2.1,
            "position": [
                -1328,
                2900
            ],
            "id": "webhook-pipeline-status",
            "name": "Pipeline Status",
            "webhookId": "36907c26-cd49-4578-abe2-2e5d5933a687"
        },
        {
            "parameters": {
                "httpMethod": "POST",
                "path": "36907c26-cd49-4578-abe2-2e5d5933a687-file-status",
                "responseMode": "responseNode",
                "options": {}
            },
            "type": "n8n-nodes-base.webhook",
            "typeVersion": 2.1,
            "position": [
                -1328,
                3100
            ],
            "id": "webhook-file-status",
            "name": "File Processing Status",
            "webhookId": "36907c26-cd49-4578-abe2-2e5d5933a687"
        },
        {
            "parameters": {
                "operation": "executeQuery",
                "query": "-- Enhanced Duplicate Detection for Enhanced Chunks with Cross-File Detection\nSELECT \n    crt.notebook_id,\n    n.notebook_title,\n    MD5(LOWER(TRIM(crt.enhanced_chunk))) AS content_hash,\n    LEFT(crt.enhanced_chunk, 200) AS chunk_preview,\n    COUNT(*) AS duplicate_count,\n    COUNT(DISTINCT crt.file_id) AS files_affected,\n    ARRAY_AGG(crt.chunk_id) AS chunk_ids,\n    ARRAY_AGG(DISTINCT crt.file_name) AS file_names,\n    CASE \n        WHEN COUNT(*) > 5 THEN 'HIGH'\n        WHEN COUNT(*) > 2 THEN 'MEDIUM'\n        ELSE 'LOW'\n    END AS impact_level,\n    CASE \n        WHEN COUNT(DISTINCT crt.file_id) > 1 THEN true\n        ELSE false\n    END AS is_cross_file_duplicate\nFROM contextual_retrieval_table crt\nJOIN notebook n ON crt.notebook_id = n.notebook_id\nWHERE crt.enhanced_chunk IS NOT NULL AND crt.enhanced_chunk != ''\nGROUP BY crt.notebook_id, n.notebook_title, MD5(LOWER(TRIM(crt.enhanced_chunk))), LEFT(crt.enhanced_chunk, 200)\nHAVING COUNT(*) > 1\nORDER BY duplicate_count DESC, n.notebook_title\nLIMIT 100;",
                "options": {}
            },
            "type": "n8n-nodes-base.postgres",
            "typeVersion": 2.6,
            "position": [
                -688,
                2096
            ],
            "id": "5f8c7720-df43-4426-b97c-6123caca4172",
            "name": "Duplicate Enhanced Query",
            "alwaysOutputData": true,
            "credentials": {
                "postgres": {
                    "id": "BkpXZhgdJu6BdpjK",
                    "name": "Local Postgres"
                }
            }
        },
        {
            "parameters": {
                "operation": "executeQuery",
                "query": "-- Enhanced Chunk Size Distribution with Quartiles & Quality Metrics\nSELECT \n    crt.notebook_id,\n    n.notebook_title,\n    COUNT(*) AS total_chunks,\n    \n    -- Size Statistics\n    MIN(LENGTH(crt.original_chunk)) AS min_chunk_size,\n    MAX(LENGTH(crt.original_chunk)) AS max_chunk_size,\n    ROUND(AVG(LENGTH(crt.original_chunk))::DECIMAL, 0) AS avg_chunk_size,\n    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY LENGTH(crt.original_chunk))::INTEGER AS median_chunk_size,\n    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY LENGTH(crt.original_chunk))::INTEGER AS p25_chunk_size,\n    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY LENGTH(crt.original_chunk))::INTEGER AS p75_chunk_size,\n    ROUND(STDDEV(LENGTH(crt.original_chunk))::DECIMAL, 0) AS stddev_chunk_size,\n    \n    -- Size Distribution Buckets\n    COUNT(*) FILTER (WHERE LENGTH(crt.original_chunk) < 100) AS very_small_chunks,\n    COUNT(*) FILTER (WHERE LENGTH(crt.original_chunk) >= 100 AND LENGTH(crt.original_chunk) < 300) AS small_chunks,\n    COUNT(*) FILTER (WHERE LENGTH(crt.original_chunk) >= 300 AND LENGTH(crt.original_chunk) < 500) AS medium_small_chunks,\n    COUNT(*) FILTER (WHERE LENGTH(crt.original_chunk) >= 500 AND LENGTH(crt.original_chunk) < 1000) AS medium_chunks,\n    COUNT(*) FILTER (WHERE LENGTH(crt.original_chunk) >= 1000 AND LENGTH(crt.original_chunk) < 2000) AS medium_large_chunks,\n    COUNT(*) FILTER (WHERE LENGTH(crt.original_chunk) >= 2000 AND LENGTH(crt.original_chunk) < 4000) AS large_chunks,\n    COUNT(*) FILTER (WHERE LENGTH(crt.original_chunk) >= 4000) AS very_large_chunks,\n    \n    -- Quality Metrics\n    ROUND(\n        (COUNT(*) FILTER (WHERE LENGTH(crt.original_chunk) BETWEEN 200 AND 2000)::DECIMAL / NULLIF(COUNT(*), 0)) * 100, \n        1\n    ) AS optimal_range_pct,\n    \n    -- Enhancement Stats\n    COUNT(*) FILTER (WHERE crt.enhanced_chunk IS NOT NULL AND crt.enhanced_chunk != '') AS enhanced_count,\n    ROUND(\n        (COUNT(*) FILTER (WHERE crt.enhanced_chunk IS NOT NULL AND crt.enhanced_chunk != '')::DECIMAL / NULLIF(COUNT(*), 0)) * 100,\n        1\n    ) AS enhancement_pct,\n    \n    -- Average Enhancement Size Increase\n    ROUND(\n        AVG(\n            CASE \n                WHEN crt.enhanced_chunk IS NOT NULL AND crt.enhanced_chunk != '' \n                THEN LENGTH(crt.enhanced_chunk) - LENGTH(crt.original_chunk)\n                ELSE NULL\n            END\n        )::DECIMAL, \n        0\n    ) AS avg_enhancement_increase\n    \nFROM contextual_retrieval_table crt\nJOIN notebook n ON crt.notebook_id = n.notebook_id\nWHERE crt.original_chunk IS NOT NULL\nGROUP BY crt.notebook_id, n.notebook_title\nORDER BY n.notebook_title;",
                "options": {}
            },
            "type": "n8n-nodes-base.postgres",
            "typeVersion": 2.6,
            "position": [
                -688,
                2448
            ],
            "id": "aa04f359-db70-49a4-803c-098a69b65429",
            "name": "Chunk Distribution Query",
            "alwaysOutputData": true,
            "credentials": {
                "postgres": {
                    "id": "BkpXZhgdJu6BdpjK",
                    "name": "Local Postgres"
                }
            }
        },
        {
            "parameters": {
                "operation": "executeQuery",
                "query": "-- Enhanced Problematic Chunks Detection with More Issue Types\nSELECT \n    crt.notebook_id,\n    n.notebook_title,\n    crt.file_name,\n    crt.file_id,\n    crt.chunk_id,\n    crt.status,\n    CASE \n        WHEN crt.original_chunk IS NULL THEN 'NULL_CONTENT'\n        WHEN crt.original_chunk = '' THEN 'EMPTY_CONTENT'\n        WHEN LENGTH(TRIM(crt.original_chunk)) = 0 THEN 'WHITESPACE_ONLY'\n        WHEN LENGTH(crt.original_chunk) < 20 THEN 'EXTREMELY_SHORT'\n        WHEN LENGTH(crt.original_chunk) < 50 THEN 'TOO_SHORT'\n        WHEN LENGTH(crt.original_chunk) > 10000 THEN 'TOO_LONG'\n        WHEN crt.original_chunk ~ '^[\\s\\d\\.\\-\\,]+$' THEN 'NUMBERS_ONLY'\n        WHEN crt.original_chunk ~ '^[^\\w\\s]+$' THEN 'SPECIAL_CHARS_ONLY'\n        ELSE 'VALID'\n    END AS issue_type,\n    LENGTH(crt.original_chunk) AS chunk_length,\n    crt.enhanced_chunk IS NOT NULL AND crt.enhanced_chunk != '' AS has_enhancement,\n    crt.retry_count,\n    crt.created_at,\n    crt.updated_at,\n    -- Actionable recommendation\n    CASE \n        WHEN crt.original_chunk IS NULL OR crt.original_chunk = '' THEN 'DELETE or RE-INGEST'\n        WHEN LENGTH(crt.original_chunk) < 50 THEN 'MERGE with adjacent chunk'\n        WHEN LENGTH(crt.original_chunk) > 10000 THEN 'SPLIT into smaller chunks'\n        ELSE 'REVIEW content quality'\n    END AS recommendation\nFROM contextual_retrieval_table crt\nJOIN notebook n ON crt.notebook_id = n.notebook_id\nWHERE \n    crt.original_chunk IS NULL \n    OR crt.original_chunk = '' \n    OR LENGTH(TRIM(crt.original_chunk)) = 0\n    OR LENGTH(crt.original_chunk) < 50\n    OR LENGTH(crt.original_chunk) > 10000\n    OR crt.original_chunk ~ '^[\\s\\d\\.\\-\\,]+$'\n    OR crt.original_chunk ~ '^[^\\w\\s]+$'\nORDER BY \n    CASE \n        WHEN crt.original_chunk IS NULL THEN 1\n        WHEN crt.original_chunk = '' THEN 2\n        WHEN LENGTH(crt.original_chunk) < 20 THEN 3\n        ELSE 4\n    END,\n    n.notebook_title, \n    crt.created_at DESC\nLIMIT 200;",
                "options": {}
            },
            "type": "n8n-nodes-base.postgres",
            "typeVersion": 2.6,
            "position": [
                -688,
                2704
            ],
            "id": "d9058786-40e0-4faa-91cd-03313cc219f1",
            "name": "Problematic Chunks Query",
            "alwaysOutputData": true,
            "credentials": {
                "postgres": {
                    "id": "BkpXZhgdJu6BdpjK",
                    "name": "Local Postgres"
                }
            }
        },
        {
            "parameters": {
                "operation": "executeQuery",
                "query": "-- Pipeline Status - Jobs & Stuck Detection\nSELECT \n    nfj.notebook_id,\n    n.notebook_title,\n    nfj.job_id,\n    nfj.file_id,\n    nfj.file_name,\n    nfj.file_type,\n    nfj.workflow_stage,\n    nfj.status,\n    nfj.retry_count,\n    nfj.error_description,\n    nfj.n8n_execution_id,\n    nfj.created_at,\n    nfj.updated_at,\n    \n    -- Time calculations\n    ROUND(EXTRACT(EPOCH FROM (NOW() - nfj.updated_at)) / 60, 1) AS minutes_since_update,\n    ROUND(EXTRACT(EPOCH FROM (nfj.updated_at - nfj.created_at)) / 60, 1) AS processing_time_minutes,\n    \n    -- Stuck job detection\n    CASE \n        WHEN nfj.status = 'processing' AND nfj.updated_at < NOW() - INTERVAL '30 minutes' THEN 'LIKELY_STUCK'\n        WHEN nfj.status = 'processing' AND nfj.updated_at < NOW() - INTERVAL '10 minutes' THEN 'POSSIBLY_SLOW'\n        WHEN nfj.status = 'pending' AND nfj.updated_at < NOW() - INTERVAL '2 hours' THEN 'STALE_PENDING'\n        WHEN nfj.status = 'pending' AND nfj.updated_at < NOW() - INTERVAL '30 minutes' THEN 'WAITING_LONG'\n        WHEN nfj.status = 'failed' AND nfj.retry_count >= 3 THEN 'MAX_RETRIES'\n        WHEN nfj.status = 'failed' THEN 'FAILED'\n        WHEN nfj.status = 'completed' THEN 'OK'\n        ELSE 'IN_PROGRESS'\n    END AS job_status_detail,\n    \n    -- Error categorization\n    CASE \n        WHEN nfj.error_description ILIKE '%timeout%' THEN 'TIMEOUT'\n        WHEN nfj.error_description ILIKE '%memory%' THEN 'MEMORY'\n        WHEN nfj.error_description ILIKE '%connection%' THEN 'CONNECTION'\n        WHEN nfj.error_description ILIKE '%parse%' OR nfj.error_description ILIKE '%format%' THEN 'PARSE_ERROR'\n        WHEN nfj.error_description ILIKE '%permission%' THEN 'PERMISSION'\n        WHEN nfj.error_description IS NOT NULL THEN 'OTHER'\n        ELSE NULL\n    END AS error_category\n    \nFROM notebook_file_jobs nfj\nJOIN notebook n ON nfj.notebook_id = n.notebook_id\nWHERE nfj.status IN ('pending', 'processing', 'failed')\n   OR nfj.updated_at > NOW() - INTERVAL '24 hours'\nORDER BY \n    CASE nfj.status \n        WHEN 'failed' THEN 1 \n        WHEN 'processing' THEN 2 \n        WHEN 'pending' THEN 3 \n        ELSE 4 \n    END,\n    nfj.updated_at DESC\nLIMIT 100;",
                "options": {}
            },
            "type": "n8n-nodes-base.postgres",
            "typeVersion": 2.6,
            "position": [
                -688,
                2900
            ],
            "id": "query-pipeline-status",
            "name": "Pipeline Status Query",
            "alwaysOutputData": true,
            "credentials": {
                "postgres": {
                    "id": "BkpXZhgdJu6BdpjK",
                    "name": "Local Postgres"
                }
            }
        },
        {
            "parameters": {
                "operation": "executeQuery",
                "query": "-- Per-File Processing Status with Vector & Enhancement Coverage\nSELECT \n    crt.notebook_id,\n    n.notebook_title,\n    crt.file_id,\n    crt.file_name,\n    \n    -- Chunk Statistics\n    COUNT(*) AS total_chunks,\n    COUNT(*) FILTER (WHERE crt.status = 'completed') AS completed_chunks,\n    COUNT(*) FILTER (WHERE crt.status = 'pending') AS pending_chunks,\n    COUNT(*) FILTER (WHERE crt.status = 'failed') AS failed_chunks,\n    COUNT(*) FILTER (WHERE crt.enhanced_chunk IS NOT NULL AND crt.enhanced_chunk != '') AS enhanced_chunks,\n    \n    -- Size Statistics\n    SUM(LENGTH(crt.original_chunk)) AS total_content_size,\n    ROUND(AVG(LENGTH(crt.original_chunk))::DECIMAL, 0) AS avg_chunk_size,\n    MIN(LENGTH(crt.original_chunk)) AS min_chunk_size,\n    MAX(LENGTH(crt.original_chunk)) AS max_chunk_size,\n    \n    -- Vector Count for this file\n    COALESCE(v.vector_count, 0) AS vector_count,\n    \n    -- Quality Issues Count\n    COUNT(*) FILTER (WHERE LENGTH(crt.original_chunk) < 50) AS short_chunks,\n    COUNT(*) FILTER (WHERE LENGTH(crt.original_chunk) > 5000) AS long_chunks,\n    \n    -- Computed Status\n    CASE \n        WHEN COUNT(*) = COUNT(*) FILTER (WHERE crt.status = 'completed') \n             AND COALESCE(v.vector_count, 0) >= COUNT(*) * 0.9 THEN 'FULLY_INDEXED'\n        WHEN COUNT(*) FILTER (WHERE crt.status = 'failed') > 0 THEN 'HAS_FAILURES'\n        WHEN COUNT(*) FILTER (WHERE crt.status = 'pending') > 0 THEN 'PROCESSING'\n        WHEN COALESCE(v.vector_count, 0) < COUNT(*) * 0.5 THEN 'VECTORS_MISSING'\n        WHEN COUNT(*) FILTER (WHERE crt.enhanced_chunk IS NOT NULL) < COUNT(*) THEN 'ENHANCEMENT_INCOMPLETE'\n        ELSE 'READY'\n    END AS indexing_status,\n    \n    -- Enhancement Rate\n    ROUND(\n        (COUNT(*) FILTER (WHERE crt.enhanced_chunk IS NOT NULL AND crt.enhanced_chunk != '')::DECIMAL / NULLIF(COUNT(*), 0)) * 100,\n        1\n    ) AS enhancement_pct,\n    \n    -- Timestamps\n    MIN(crt.created_at) AS first_chunk_at,\n    MAX(crt.updated_at) AS last_update_at\n\nFROM contextual_retrieval_table crt\nJOIN notebook n ON crt.notebook_id = n.notebook_id\nLEFT JOIN (\n    SELECT \n        metadata->>'notebook_id' AS notebook_id,\n        metadata->>'file_id' AS file_id,\n        COUNT(*) AS vector_count\n    FROM documents\n    WHERE metadata->>'notebook_id' IS NOT NULL\n    GROUP BY metadata->>'notebook_id', metadata->>'file_id'\n) v ON crt.notebook_id::TEXT = v.notebook_id AND crt.file_id = v.file_id\n\nGROUP BY crt.notebook_id, n.notebook_title, crt.file_id, crt.file_name, v.vector_count\nORDER BY \n    CASE \n        WHEN COUNT(*) FILTER (WHERE crt.status = 'failed') > 0 THEN 1\n        WHEN COUNT(*) FILTER (WHERE crt.status = 'pending') > 0 THEN 2\n        ELSE 3\n    END,\n    n.notebook_title, \n    crt.file_name;",
                "options": {}
            },
            "type": "n8n-nodes-base.postgres",
            "typeVersion": 2.6,
            "position": [
                -688,
                3100
            ],
            "id": "query-file-status",
            "name": "File Status Query",
            "alwaysOutputData": true,
            "credentials": {
                "postgres": {
                    "id": "BkpXZhgdJu6BdpjK",
                    "name": "Local Postgres"
                }
            }
        }
    ],
    "connections": {
        "Duplicate Chunks Query": {
            "main": [
                [
                    {
                        "node": "Respond to Webhook13",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Health Overview Query": {
            "main": [
                [
                    {
                        "node": "Respond to Webhook15",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Chunk size distribution per notebook": {
            "main": [
                [
                    {
                        "node": "Chunk Distribution Query",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Comprehensive notebook health overview": {
            "main": [
                [
                    {
                        "node": "Health Overview Query",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Find duplicate chunks": {
            "main": [
                [
                    {
                        "node": "Duplicate Chunks Query",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Find duplicate enhanced chunks": {
            "main": [
                [
                    {
                        "node": "Duplicate Enhanced Query",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Problematic chunks detection": {
            "main": [
                [
                    {
                        "node": "Problematic Chunks Query",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Duplicate Enhanced Query": {
            "main": [
                [
                    {
                        "node": "Respond to Webhook14",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Chunk Distribution Query": {
            "main": [
                [
                    {
                        "node": "Respond to Webhook16",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Problematic Chunks Query": {
            "main": [
                [
                    {
                        "node": "Respond to Webhook17",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Pipeline Status": {
            "main": [
                [
                    {
                        "node": "Pipeline Status Query",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "File Processing Status": {
            "main": [
                [
                    {
                        "node": "File Status Query",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Pipeline Status Query": {
            "main": [
                [
                    {
                        "node": "Respond to Webhook - Pipeline",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "File Status Query": {
            "main": [
                [
                    {
                        "node": "Respond to Webhook - Files",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        }
    },
    "pinData": {},
    "meta": {
        "instanceId": "764abf43019c66c51279befb0403092fc7e1beab1eff90fa2cd5f93d91d18c8a"
    }
}