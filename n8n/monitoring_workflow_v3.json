{
    "nodes": [
        {
            "parameters": {
                "content": "## Notebook Monitor V3 - Single Notebook Focus\n\n**Key Fixes:**\n- ðŸŽ¯ Filters by notebook_id from request body\n- âœ… Accurate enhancement detection (checks if actually different)\n- âœ… Only shows truly stuck/failed jobs\n- âœ… Correct completion status logic\n\n**Request Format:**\n```json\n{ \"notebook_id\": \"uuid\" }\n```",
                "height": 1600,
                "width": 2200
            },
            "type": "n8n-nodes-base.stickyNote",
            "typeVersion": 1,
            "position": [
                -1600,
                1676
            ],
            "id": "66607fd5-81ee-47b4-a23e-a28a1b53da95",
            "name": "Sticky Note17"
        },
        {
            "parameters": {
                "options": {}
            },
            "type": "n8n-nodes-base.respondToWebhook",
            "typeVersion": 1.4,
            "position": [
                48,
                1920
            ],
            "id": "f31ad120-9ff5-4258-8205-5c9e48baf744",
            "name": "Respond - Duplicates"
        },
        {
            "parameters": {
                "options": {}
            },
            "type": "n8n-nodes-base.respondToWebhook",
            "typeVersion": 1.4,
            "position": [
                48,
                2096
            ],
            "id": "6622781e-4365-4fcd-a83c-2cec206725b9",
            "name": "Respond - Enhanced Duplicates"
        },
        {
            "parameters": {
                "options": {}
            },
            "type": "n8n-nodes-base.respondToWebhook",
            "typeVersion": 1.4,
            "position": [
                48,
                2256
            ],
            "id": "88ad3d21-c098-40da-a843-d6edac72037d",
            "name": "Respond - Health"
        },
        {
            "parameters": {
                "options": {}
            },
            "type": "n8n-nodes-base.respondToWebhook",
            "typeVersion": 1.4,
            "position": [
                48,
                2448
            ],
            "id": "d579a744-af48-4b48-843a-a3617beb8684",
            "name": "Respond - Distribution"
        },
        {
            "parameters": {
                "options": {}
            },
            "type": "n8n-nodes-base.respondToWebhook",
            "typeVersion": 1.4,
            "position": [
                48,
                2704
            ],
            "id": "6b89ce6d-c94f-439f-8ff5-047fa30eda43",
            "name": "Respond - Quality"
        },
        {
            "parameters": {
                "options": {}
            },
            "type": "n8n-nodes-base.respondToWebhook",
            "typeVersion": 1.4,
            "position": [
                48,
                2900
            ],
            "id": "respond-pipeline-status",
            "name": "Respond - Pipeline"
        },
        {
            "parameters": {
                "options": {}
            },
            "type": "n8n-nodes-base.respondToWebhook",
            "typeVersion": 1.4,
            "position": [
                48,
                3100
            ],
            "id": "respond-file-status",
            "name": "Respond - Files"
        },
        {
            "parameters": {
                "operation": "executeQuery",
                "query": "-- DUPLICATE CHUNKS for SPECIFIC NOTEBOOK\n-- Uses content hash for accurate detection\nSELECT \n    notebook_id,\n    MD5(LOWER(TRIM(original_chunk))) AS content_hash,\n    LEFT(original_chunk, 200) AS chunk_preview,\n    COUNT(*) AS duplicate_count,\n    COUNT(DISTINCT file_id) AS files_affected,\n    ARRAY_AGG(DISTINCT file_name) AS file_names,\n    ARRAY_AGG(chunk_id) AS chunk_ids,\n    MIN(created_at) AS first_created,\n    MAX(created_at) AS last_created,\n    CASE \n        WHEN COUNT(*) > 5 THEN 'HIGH'\n        WHEN COUNT(*) > 2 THEN 'MEDIUM'\n        ELSE 'LOW'\n    END AS impact_level,\n    COUNT(DISTINCT file_id) > 1 AS is_cross_file_duplicate\nFROM contextual_retrieval_table\nWHERE notebook_id = '{{ $json.body.notebook_id }}'::uuid\n  AND original_chunk IS NOT NULL \n  AND LENGTH(TRIM(original_chunk)) > 10\nGROUP BY notebook_id, MD5(LOWER(TRIM(original_chunk))), LEFT(original_chunk, 200)\nHAVING COUNT(*) > 1\nORDER BY duplicate_count DESC\nLIMIT 50;",
                "options": {}
            },
            "type": "n8n-nodes-base.postgres",
            "typeVersion": 2.6,
            "position": [
                -500,
                1920
            ],
            "id": "aa939409-6bc0-494f-89b3-9401ad38c91d",
            "name": "Query - Duplicates",
            "alwaysOutputData": true,
            "credentials": {
                "postgres": {
                    "id": "BkpXZhgdJu6BdpjK",
                    "name": "Local Postgres"
                }
            }
        },
        {
            "parameters": {
                "operation": "executeQuery",
                "query": "-- HEALTH OVERVIEW for SPECIFIC NOTEBOOK\n-- Fixed: Only counts REAL enhancements (different from original)\nWITH notebook_data AS (\n    SELECT * FROM notebook WHERE notebook_id = '{{ $json.body.notebook_id }}'::uuid\n),\nchunk_stats AS (\n    SELECT \n        COUNT(*) AS total_chunks,\n        COUNT(*) FILTER (WHERE status = 'pending') AS pending_chunks,\n        COUNT(*) FILTER (WHERE status = 'completed') AS completed_chunks,\n        COUNT(*) FILTER (WHERE status = 'failed') AS failed_chunks,\n        -- FIXED: Only count as enhanced if enhanced_chunk EXISTS and is DIFFERENT from original\n        COUNT(*) FILTER (\n            WHERE enhanced_chunk IS NOT NULL \n            AND enhanced_chunk != '' \n            AND enhanced_chunk != original_chunk\n            AND LENGTH(enhanced_chunk) > LENGTH(original_chunk)\n        ) AS truly_enhanced_chunks\n    FROM contextual_retrieval_table\n    WHERE notebook_id = '{{ $json.body.notebook_id }}'::uuid\n),\njob_stats AS (\n    SELECT \n        COUNT(*) AS total_jobs,\n        COUNT(*) FILTER (WHERE status = 'pending') AS pending_jobs,\n        COUNT(*) FILTER (WHERE status = 'processing') AS processing_jobs,\n        COUNT(*) FILTER (WHERE status = 'completed') AS completed_jobs,\n        COUNT(*) FILTER (WHERE status = 'failed') AS failed_jobs\n    FROM notebook_file_jobs\n    WHERE notebook_id = '{{ $json.body.notebook_id }}'::uuid\n),\ndoc_stats AS (\n    SELECT \n        COUNT(*) AS total_docs,\n        COUNT(*) FILTER (WHERE document_type = 'tabular') AS tabular_docs,\n        COUNT(*) FILTER (WHERE document_type = 'non_tabular') AS non_tabular_docs\n    FROM document_records\n    WHERE notebook_id = '{{ $json.body.notebook_id }}'::uuid\n),\nvector_stats AS (\n    SELECT \n        COUNT(*) AS vector_count,\n        COUNT(*) FILTER (WHERE embedding IS NULL) AS missing_embeddings\n    FROM documents\n    WHERE metadata->>'notebook_id' = '{{ $json.body.notebook_id }}'\n),\ndup_stats AS (\n    SELECT COUNT(*) AS duplicate_groups\n    FROM (\n        SELECT 1\n        FROM contextual_retrieval_table\n        WHERE notebook_id = '{{ $json.body.notebook_id }}'::uuid\n          AND original_chunk IS NOT NULL\n          AND LENGTH(TRIM(original_chunk)) > 10\n        GROUP BY MD5(LOWER(TRIM(original_chunk)))\n        HAVING COUNT(*) > 1\n    ) d\n),\nquality_stats AS (\n    SELECT COUNT(*) AS problematic_count\n    FROM contextual_retrieval_table\n    WHERE notebook_id = '{{ $json.body.notebook_id }}'::uuid\n      AND (\n          original_chunk IS NULL \n          OR original_chunk = '' \n          OR LENGTH(TRIM(original_chunk)) < 20\n      )\n),\nchat_stats AS (\n    SELECT \n        COUNT(*) AS message_count,\n        COUNT(DISTINCT user_id) AS unique_users\n    FROM chat_history\n    WHERE notebook_id = '{{ $json.body.notebook_id }}'::uuid\n)\nSELECT \n    n.notebook_id,\n    n.notebook_title,\n    n.number_of_documents,\n    n.created_at,\n    n.updated_at,\n    \n    -- Document Stats\n    COALESCE(ds.total_docs, 0) AS actual_document_records,\n    COALESCE(ds.tabular_docs, 0) AS tabular_documents,\n    COALESCE(ds.non_tabular_docs, 0) AS non_tabular_documents,\n    \n    -- Chunk Stats\n    COALESCE(cs.total_chunks, 0) AS total_chunks,\n    COALESCE(cs.pending_chunks, 0) AS pending_chunks,\n    COALESCE(cs.completed_chunks, 0) AS completed_chunks,\n    COALESCE(cs.failed_chunks, 0) AS failed_chunks,\n    COALESCE(cs.truly_enhanced_chunks, 0) AS enhanced_chunks,\n    \n    -- Enhancement Coverage (FIXED)\n    CASE \n        WHEN COALESCE(cs.total_chunks, 0) = 0 THEN 0\n        ELSE ROUND((COALESCE(cs.truly_enhanced_chunks, 0)::DECIMAL / cs.total_chunks) * 100, 1)\n    END AS enhancement_coverage_pct,\n    \n    -- Job Stats\n    COALESCE(js.total_jobs, 0) AS total_jobs,\n    COALESCE(js.pending_jobs, 0) AS pending_jobs,\n    COALESCE(js.processing_jobs, 0) AS processing_jobs,\n    COALESCE(js.completed_jobs, 0) AS completed_jobs,\n    COALESCE(js.failed_jobs, 0) AS failed_jobs,\n    \n    -- Job Success Rate (FIXED: only calculate if there are jobs)\n    CASE \n        WHEN COALESCE(js.total_jobs, 0) = 0 THEN 100\n        WHEN js.pending_jobs + js.processing_jobs > 0 THEN \n            ROUND((js.completed_jobs::DECIMAL / (js.completed_jobs + js.failed_jobs + 0.001)) * 100, 1)\n        ELSE ROUND((js.completed_jobs::DECIMAL / js.total_jobs) * 100, 1)\n    END AS job_success_rate_pct,\n    \n    -- Vector Stats\n    COALESCE(vs.vector_count, 0) AS vector_documents,\n    COALESCE(vs.missing_embeddings, 0) AS vectors_missing_embedding,\n    \n    -- Quality Stats\n    COALESCE(dups.duplicate_groups, 0) AS duplicate_chunk_groups,\n    COALESCE(qs.problematic_count, 0) AS problematic_chunks,\n    \n    -- Chat Stats\n    COALESCE(chats.message_count, 0) AS chat_messages,\n    COALESCE(chats.unique_users, 0) AS unique_chat_users,\n    \n    -- Orphan vectors (simplified)\n    0 AS orphan_vectors,\n    0 AS total_duplicate_chunks,\n    \n    -- HEALTH STATUS (FIXED logic)\n    CASE\n        WHEN COALESCE(js.failed_jobs, 0) > 0 AND js.failed_jobs > js.completed_jobs THEN 'CRITICAL'\n        WHEN COALESCE(cs.failed_chunks, 0) > COALESCE(cs.total_chunks, 0) * 0.1 THEN 'CRITICAL'\n        WHEN COALESCE(js.pending_jobs, 0) + COALESCE(js.processing_jobs, 0) > 0 THEN 'SYNCING'\n        WHEN COALESCE(cs.pending_chunks, 0) > 0 THEN 'SYNCING'\n        WHEN COALESCE(dups.duplicate_groups, 0) > 10 THEN 'WARNING'\n        WHEN COALESCE(qs.problematic_count, 0) > 5 THEN 'WARNING'\n        ELSE 'HEALTHY'\n    END AS health_status\n\nFROM notebook_data n\nCROSS JOIN chunk_stats cs\nCROSS JOIN job_stats js\nCROSS JOIN doc_stats ds\nCROSS JOIN vector_stats vs\nCROSS JOIN dup_stats dups\nCROSS JOIN quality_stats qs\nCROSS JOIN chat_stats chats;",
                "options": {}
            },
            "type": "n8n-nodes-base.postgres",
            "typeVersion": 2.6,
            "position": [
                -500,
                2256
            ],
            "id": "600822d2-19e5-4ce6-a52b-56438ba3fda6",
            "name": "Query - Health",
            "alwaysOutputData": true,
            "credentials": {
                "postgres": {
                    "id": "BkpXZhgdJu6BdpjK",
                    "name": "Local Postgres"
                }
            }
        },
        {
            "parameters": {
                "httpMethod": "POST",
                "path": "36907c26-cd49-4578-abe2-2e5d5933a687-Chunk-size-distribution-per-notebook",
                "responseMode": "responseNode",
                "options": {}
            },
            "type": "n8n-nodes-base.webhook",
            "typeVersion": 2.1,
            "position": [
                -1200,
                2448
            ],
            "id": "62a07c14-88b7-4c50-89b3-215860e69181",
            "name": "Webhook - Distribution",
            "webhookId": "36907c26-cd49-4578-abe2-2e5d5933a687"
        },
        {
            "parameters": {
                "httpMethod": "POST",
                "path": "36907c26-cd49-4578-abe2-2e5d5933a687-Comprehensive-notebook-health-overview",
                "responseMode": "responseNode",
                "options": {}
            },
            "type": "n8n-nodes-base.webhook",
            "typeVersion": 2.1,
            "position": [
                -1200,
                2256
            ],
            "id": "1b19f131-8e04-4296-8027-2b8f8551c752",
            "name": "Webhook - Health",
            "webhookId": "36907c26-cd49-4578-abe2-2e5d5933a687"
        },
        {
            "parameters": {
                "httpMethod": "POST",
                "path": "36907c26-cd49-4578-abe2-2e5d5933a687- duplicate-chunks-within-the-same-notebook",
                "responseMode": "responseNode",
                "options": {}
            },
            "type": "n8n-nodes-base.webhook",
            "typeVersion": 2.1,
            "position": [
                -1200,
                1920
            ],
            "id": "6ff7e2e1-dcd6-4d9a-b4bf-402f4e18af91",
            "name": "Webhook - Duplicates",
            "webhookId": "36907c26-cd49-4578-abe2-2e5d5933a687"
        },
        {
            "parameters": {
                "httpMethod": "POST",
                "path": "36907c26-cd49-4578-abe2-2e5d5933a687-Find-duplicate-enhanced-chunks-within-the-same-notebook",
                "responseMode": "responseNode",
                "options": {}
            },
            "type": "n8n-nodes-base.webhook",
            "typeVersion": 2.1,
            "position": [
                -1200,
                2096
            ],
            "id": "9ed69a5a-b1ed-46fb-9686-eca6d72ddda6",
            "name": "Webhook - Enhanced Duplicates",
            "webhookId": "36907c26-cd49-4578-abe2-2e5d5933a687"
        },
        {
            "parameters": {
                "httpMethod": "POST",
                "path": "36907c26-cd49-4578-abe2-2e5d5933a687-Empty-or-problematic-chunks-detection",
                "responseMode": "responseNode",
                "options": {}
            },
            "type": "n8n-nodes-base.webhook",
            "typeVersion": 2.1,
            "position": [
                -1200,
                2704
            ],
            "id": "7c487b09-522e-4fe7-bc97-20e19820ad13",
            "name": "Webhook - Quality",
            "webhookId": "36907c26-cd49-4578-abe2-2e5d5933a687"
        },
        {
            "parameters": {
                "httpMethod": "POST",
                "path": "36907c26-cd49-4578-abe2-2e5d5933a687-pipeline-status",
                "responseMode": "responseNode",
                "options": {}
            },
            "type": "n8n-nodes-base.webhook",
            "typeVersion": 2.1,
            "position": [
                -1200,
                2900
            ],
            "id": "webhook-pipeline-status",
            "name": "Webhook - Pipeline",
            "webhookId": "36907c26-cd49-4578-abe2-2e5d5933a687"
        },
        {
            "parameters": {
                "httpMethod": "POST",
                "path": "36907c26-cd49-4578-abe2-2e5d5933a687-file-status",
                "responseMode": "responseNode",
                "options": {}
            },
            "type": "n8n-nodes-base.webhook",
            "typeVersion": 2.1,
            "position": [
                -1200,
                3100
            ],
            "id": "webhook-file-status",
            "name": "Webhook - Files",
            "webhookId": "36907c26-cd49-4578-abe2-2e5d5933a687"
        },
        {
            "parameters": {
                "operation": "executeQuery",
                "query": "-- DUPLICATE ENHANCED CHUNKS for SPECIFIC NOTEBOOK\n-- Only finds REAL enhanced duplicates\nSELECT \n    notebook_id,\n    MD5(LOWER(TRIM(enhanced_chunk))) AS content_hash,\n    LEFT(enhanced_chunk, 200) AS chunk_preview,\n    COUNT(*) AS duplicate_count,\n    COUNT(DISTINCT file_id) AS files_affected,\n    ARRAY_AGG(chunk_id) AS chunk_ids,\n    ARRAY_AGG(DISTINCT file_name) AS file_names,\n    CASE \n        WHEN COUNT(*) > 5 THEN 'HIGH'\n        WHEN COUNT(*) > 2 THEN 'MEDIUM'\n        ELSE 'LOW'\n    END AS impact_level,\n    COUNT(DISTINCT file_id) > 1 AS is_cross_file_duplicate\nFROM contextual_retrieval_table\nWHERE notebook_id = '{{ $json.body.notebook_id }}'::uuid\n  AND enhanced_chunk IS NOT NULL \n  AND enhanced_chunk != ''\n  AND enhanced_chunk != original_chunk\n  AND LENGTH(enhanced_chunk) > 50\nGROUP BY notebook_id, MD5(LOWER(TRIM(enhanced_chunk))), LEFT(enhanced_chunk, 200)\nHAVING COUNT(*) > 1\nORDER BY duplicate_count DESC\nLIMIT 50;",
                "options": {}
            },
            "type": "n8n-nodes-base.postgres",
            "typeVersion": 2.6,
            "position": [
                -500,
                2096
            ],
            "id": "5f8c7720-df43-4426-b97c-6123caca4172",
            "name": "Query - Enhanced Duplicates",
            "alwaysOutputData": true,
            "credentials": {
                "postgres": {
                    "id": "BkpXZhgdJu6BdpjK",
                    "name": "Local Postgres"
                }
            }
        },
        {
            "parameters": {
                "operation": "executeQuery",
                "query": "-- CHUNK SIZE DISTRIBUTION for SPECIFIC NOTEBOOK\nSELECT \n    notebook_id,\n    COUNT(*) AS total_chunks,\n    \n    -- Size Statistics\n    MIN(LENGTH(original_chunk)) AS min_chunk_size,\n    MAX(LENGTH(original_chunk)) AS max_chunk_size,\n    ROUND(AVG(LENGTH(original_chunk))::DECIMAL, 0) AS avg_chunk_size,\n    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY LENGTH(original_chunk))::INTEGER AS median_chunk_size,\n    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY LENGTH(original_chunk))::INTEGER AS p25_chunk_size,\n    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY LENGTH(original_chunk))::INTEGER AS p75_chunk_size,\n    ROUND(STDDEV(LENGTH(original_chunk))::DECIMAL, 0) AS stddev_chunk_size,\n    \n    -- Size Distribution (7 buckets)\n    COUNT(*) FILTER (WHERE LENGTH(original_chunk) < 100) AS very_small_chunks,\n    COUNT(*) FILTER (WHERE LENGTH(original_chunk) BETWEEN 100 AND 299) AS small_chunks,\n    COUNT(*) FILTER (WHERE LENGTH(original_chunk) BETWEEN 300 AND 499) AS medium_small_chunks,\n    COUNT(*) FILTER (WHERE LENGTH(original_chunk) BETWEEN 500 AND 999) AS medium_chunks,\n    COUNT(*) FILTER (WHERE LENGTH(original_chunk) BETWEEN 1000 AND 1999) AS medium_large_chunks,\n    COUNT(*) FILTER (WHERE LENGTH(original_chunk) BETWEEN 2000 AND 3999) AS large_chunks,\n    COUNT(*) FILTER (WHERE LENGTH(original_chunk) >= 4000) AS very_large_chunks,\n    \n    -- Optimal range (chunks between 200-2000 chars)\n    ROUND(\n        (COUNT(*) FILTER (WHERE LENGTH(original_chunk) BETWEEN 200 AND 2000)::DECIMAL / NULLIF(COUNT(*), 0)) * 100, \n        1\n    ) AS optimal_range_pct,\n    \n    -- FIXED Enhancement count: only count REAL enhancements\n    COUNT(*) FILTER (\n        WHERE enhanced_chunk IS NOT NULL \n        AND enhanced_chunk != '' \n        AND enhanced_chunk != original_chunk\n        AND LENGTH(enhanced_chunk) > LENGTH(original_chunk)\n    ) AS enhanced_count,\n    \n    -- Enhancement percentage\n    ROUND(\n        (COUNT(*) FILTER (\n            WHERE enhanced_chunk IS NOT NULL \n            AND enhanced_chunk != '' \n            AND enhanced_chunk != original_chunk\n        )::DECIMAL / NULLIF(COUNT(*), 0)) * 100,\n        1\n    ) AS enhancement_pct,\n    \n    -- Average enhancement size increase\n    COALESCE(\n        ROUND(\n            AVG(\n                CASE \n                    WHEN enhanced_chunk IS NOT NULL \n                    AND enhanced_chunk != '' \n                    AND enhanced_chunk != original_chunk\n                    AND LENGTH(enhanced_chunk) > LENGTH(original_chunk)\n                    THEN LENGTH(enhanced_chunk) - LENGTH(original_chunk)\n                    ELSE NULL\n                END\n            )::DECIMAL, \n            0\n        ),\n        0\n    ) AS avg_enhancement_increase\n    \nFROM contextual_retrieval_table\nWHERE notebook_id = '{{ $json.body.notebook_id }}'::uuid\n  AND original_chunk IS NOT NULL\nGROUP BY notebook_id;",
                "options": {}
            },
            "type": "n8n-nodes-base.postgres",
            "typeVersion": 2.6,
            "position": [
                -500,
                2448
            ],
            "id": "aa04f359-db70-49a4-803c-098a69b65429",
            "name": "Query - Distribution",
            "alwaysOutputData": true,
            "credentials": {
                "postgres": {
                    "id": "BkpXZhgdJu6BdpjK",
                    "name": "Local Postgres"
                }
            }
        },
        {
            "parameters": {
                "operation": "executeQuery",
                "query": "-- PROBLEMATIC CHUNKS for SPECIFIC NOTEBOOK\nSELECT \n    notebook_id,\n    file_name,\n    file_id,\n    chunk_id,\n    status,\n    CASE \n        WHEN original_chunk IS NULL THEN 'NULL_CONTENT'\n        WHEN original_chunk = '' THEN 'EMPTY_CONTENT'\n        WHEN LENGTH(TRIM(original_chunk)) = 0 THEN 'WHITESPACE_ONLY'\n        WHEN LENGTH(original_chunk) < 20 THEN 'EXTREMELY_SHORT'\n        WHEN LENGTH(original_chunk) < 50 THEN 'TOO_SHORT'\n        WHEN LENGTH(original_chunk) > 10000 THEN 'TOO_LONG'\n        ELSE 'UNKNOWN'\n    END AS issue_type,\n    COALESCE(LENGTH(original_chunk), 0) AS chunk_length,\n    (\n        enhanced_chunk IS NOT NULL \n        AND enhanced_chunk != '' \n        AND enhanced_chunk != original_chunk\n    ) AS has_enhancement,\n    retry_count,\n    created_at,\n    updated_at,\n    CASE \n        WHEN original_chunk IS NULL OR original_chunk = '' THEN 'DELETE or RE-INGEST file'\n        WHEN LENGTH(original_chunk) < 50 THEN 'Consider merging with adjacent chunks'\n        WHEN LENGTH(original_chunk) > 10000 THEN 'Split into smaller chunks'\n        ELSE 'Review content quality'\n    END AS recommendation\nFROM contextual_retrieval_table\nWHERE notebook_id = '{{ $json.body.notebook_id }}'::uuid\n  AND (\n      original_chunk IS NULL \n      OR original_chunk = '' \n      OR LENGTH(TRIM(original_chunk)) < 20\n      OR LENGTH(original_chunk) > 10000\n  )\nORDER BY \n    CASE \n        WHEN original_chunk IS NULL THEN 1\n        WHEN original_chunk = '' THEN 2\n        WHEN LENGTH(original_chunk) < 20 THEN 3\n        ELSE 4\n    END,\n    created_at DESC\nLIMIT 100;",
                "options": {}
            },
            "type": "n8n-nodes-base.postgres",
            "typeVersion": 2.6,
            "position": [
                -500,
                2704
            ],
            "id": "d9058786-40e0-4faa-91cd-03313cc219f1",
            "name": "Query - Quality",
            "alwaysOutputData": true,
            "credentials": {
                "postgres": {
                    "id": "BkpXZhgdJu6BdpjK",
                    "name": "Local Postgres"
                }
            }
        },
        {
            "parameters": {
                "operation": "executeQuery",
                "query": "-- PIPELINE STATUS for SPECIFIC NOTEBOOK\n-- FIXED: Only shows ACTUAL issues (pending/processing/failed)\n-- Does NOT show completed jobs\nSELECT \n    job_id,\n    notebook_id,\n    file_id,\n    file_name,\n    file_type,\n    workflow_stage,\n    status,\n    retry_count,\n    error_description,\n    n8n_execution_id,\n    created_at,\n    updated_at,\n    \n    -- Time calculations\n    ROUND(EXTRACT(EPOCH FROM (NOW() - updated_at)) / 60, 1) AS minutes_since_update,\n    ROUND(EXTRACT(EPOCH FROM (updated_at - created_at)) / 60, 1) AS processing_time_minutes,\n    \n    -- FIXED: Job status detection\n    CASE \n        WHEN status = 'failed' AND retry_count >= 3 THEN 'MAX_RETRIES'\n        WHEN status = 'failed' THEN 'FAILED'\n        WHEN status = 'processing' AND updated_at < NOW() - INTERVAL '30 minutes' THEN 'LIKELY_STUCK'\n        WHEN status = 'processing' THEN 'IN_PROGRESS'\n        WHEN status = 'pending' AND updated_at < NOW() - INTERVAL '1 hour' THEN 'STALE_PENDING'\n        WHEN status = 'pending' THEN 'QUEUED'\n        WHEN status = 'completed' THEN 'OK'\n        ELSE 'UNKNOWN'\n    END AS job_status_detail,\n    \n    -- Error categorization\n    CASE \n        WHEN error_description ILIKE '%timeout%' THEN 'TIMEOUT'\n        WHEN error_description ILIKE '%memory%' THEN 'MEMORY'\n        WHEN error_description ILIKE '%connection%' THEN 'CONNECTION'\n        WHEN error_description ILIKE '%parse%' OR error_description ILIKE '%format%' THEN 'PARSE_ERROR'\n        WHEN error_description IS NOT NULL AND error_description != '' THEN 'OTHER'\n        ELSE NULL\n    END AS error_category\n    \nFROM notebook_file_jobs\nWHERE notebook_id = '{{ $json.body.notebook_id }}'::uuid\n  -- FIXED: Only show pending, processing, or failed jobs (not completed!)\n  AND status IN ('pending', 'processing', 'failed')\nORDER BY \n    CASE status \n        WHEN 'failed' THEN 1 \n        WHEN 'processing' THEN 2 \n        WHEN 'pending' THEN 3 \n        ELSE 4 \n    END,\n    updated_at DESC\nLIMIT 50;",
                "options": {}
            },
            "type": "n8n-nodes-base.postgres",
            "typeVersion": 2.6,
            "position": [
                -500,
                2900
            ],
            "id": "query-pipeline-status",
            "name": "Query - Pipeline",
            "alwaysOutputData": true,
            "credentials": {
                "postgres": {
                    "id": "BkpXZhgdJu6BdpjK",
                    "name": "Local Postgres"
                }
            }
        },
        {
            "parameters": {
                "operation": "executeQuery",
                "query": "-- FILE STATUS for SPECIFIC NOTEBOOK\n-- Shows per-file processing status\nWITH file_chunks AS (\n    SELECT \n        file_id,\n        file_name,\n        COUNT(*) AS total_chunks,\n        COUNT(*) FILTER (WHERE status = 'completed') AS completed_chunks,\n        COUNT(*) FILTER (WHERE status = 'pending') AS pending_chunks,\n        COUNT(*) FILTER (WHERE status = 'failed') AS failed_chunks,\n        -- FIXED: Only count REAL enhancements\n        COUNT(*) FILTER (\n            WHERE enhanced_chunk IS NOT NULL \n            AND enhanced_chunk != '' \n            AND enhanced_chunk != original_chunk\n            AND LENGTH(enhanced_chunk) > LENGTH(original_chunk)\n        ) AS enhanced_chunks,\n        SUM(LENGTH(original_chunk)) AS total_content_size,\n        ROUND(AVG(LENGTH(original_chunk))::DECIMAL, 0) AS avg_chunk_size,\n        MIN(LENGTH(original_chunk)) AS min_chunk_size,\n        MAX(LENGTH(original_chunk)) AS max_chunk_size,\n        COUNT(*) FILTER (WHERE LENGTH(original_chunk) < 50) AS short_chunks,\n        COUNT(*) FILTER (WHERE LENGTH(original_chunk) > 5000) AS long_chunks,\n        MIN(created_at) AS first_chunk_at,\n        MAX(updated_at) AS last_update_at\n    FROM contextual_retrieval_table\n    WHERE notebook_id = '{{ $json.body.notebook_id }}'::uuid\n    GROUP BY file_id, file_name\n),\nfile_vectors AS (\n    SELECT \n        metadata->>'file_id' AS file_id,\n        COUNT(*) AS vector_count\n    FROM documents\n    WHERE metadata->>'notebook_id' = '{{ $json.body.notebook_id }}'\n    GROUP BY metadata->>'file_id'\n)\nSELECT \n    '{{ $json.body.notebook_id }}'::uuid AS notebook_id,\n    fc.file_id,\n    fc.file_name,\n    fc.total_chunks,\n    fc.completed_chunks,\n    fc.pending_chunks,\n    fc.failed_chunks,\n    fc.enhanced_chunks,\n    fc.total_content_size,\n    fc.avg_chunk_size,\n    fc.min_chunk_size,\n    fc.max_chunk_size,\n    COALESCE(fv.vector_count, 0) AS vector_count,\n    fc.short_chunks,\n    fc.long_chunks,\n    \n    -- FIXED: Accurate indexing status\n    CASE \n        WHEN fc.failed_chunks > 0 THEN 'HAS_FAILURES'\n        WHEN fc.pending_chunks > 0 THEN 'PROCESSING'\n        WHEN fc.completed_chunks = fc.total_chunks AND COALESCE(fv.vector_count, 0) >= fc.total_chunks THEN 'FULLY_INDEXED'\n        WHEN fc.completed_chunks = fc.total_chunks AND COALESCE(fv.vector_count, 0) < fc.total_chunks THEN 'VECTORS_PENDING'\n        WHEN fc.completed_chunks = fc.total_chunks THEN 'COMPLETED'\n        ELSE 'IN_PROGRESS'\n    END AS indexing_status,\n    \n    -- Enhancement percentage\n    CASE \n        WHEN fc.total_chunks = 0 THEN 0\n        ELSE ROUND((fc.enhanced_chunks::DECIMAL / fc.total_chunks) * 100, 1)\n    END AS enhancement_pct,\n    \n    fc.first_chunk_at,\n    fc.last_update_at\n    \nFROM file_chunks fc\nLEFT JOIN file_vectors fv ON fc.file_id = fv.file_id\nORDER BY \n    CASE \n        WHEN fc.failed_chunks > 0 THEN 1\n        WHEN fc.pending_chunks > 0 THEN 2\n        ELSE 3\n    END,\n    fc.file_name;",
                "options": {}
            },
            "type": "n8n-nodes-base.postgres",
            "typeVersion": 2.6,
            "position": [
                -500,
                3100
            ],
            "id": "query-file-status",
            "name": "Query - Files",
            "alwaysOutputData": true,
            "credentials": {
                "postgres": {
                    "id": "BkpXZhgdJu6BdpjK",
                    "name": "Local Postgres"
                }
            }
        }
    ],
    "connections": {
        "Query - Duplicates": {
            "main": [
                [
                    {
                        "node": "Respond - Duplicates",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Query - Health": {
            "main": [
                [
                    {
                        "node": "Respond - Health",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Webhook - Distribution": {
            "main": [
                [
                    {
                        "node": "Query - Distribution",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Webhook - Health": {
            "main": [
                [
                    {
                        "node": "Query - Health",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Webhook - Duplicates": {
            "main": [
                [
                    {
                        "node": "Query - Duplicates",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Webhook - Enhanced Duplicates": {
            "main": [
                [
                    {
                        "node": "Query - Enhanced Duplicates",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Webhook - Quality": {
            "main": [
                [
                    {
                        "node": "Query - Quality",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Query - Enhanced Duplicates": {
            "main": [
                [
                    {
                        "node": "Respond - Enhanced Duplicates",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Query - Distribution": {
            "main": [
                [
                    {
                        "node": "Respond - Distribution",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Query - Quality": {
            "main": [
                [
                    {
                        "node": "Respond - Quality",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Webhook - Pipeline": {
            "main": [
                [
                    {
                        "node": "Query - Pipeline",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Webhook - Files": {
            "main": [
                [
                    {
                        "node": "Query - Files",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Query - Pipeline": {
            "main": [
                [
                    {
                        "node": "Respond - Pipeline",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        },
        "Query - Files": {
            "main": [
                [
                    {
                        "node": "Respond - Files",
                        "type": "main",
                        "index": 0
                    }
                ]
            ]
        }
    },
    "pinData": {},
    "meta": {
        "instanceId": "764abf43019c66c51279befb0403092fc7e1beab1eff90fa2cd5f93d91d18c8a"
    }
}